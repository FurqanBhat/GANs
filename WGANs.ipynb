{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPrgPRCFSjbq8Y1fFj9KJuP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FurqanBhat/GANs/blob/main/WGANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kSyd9AmvP8z"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import MNIST\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "import torch.autograd as autograd\n",
        "\n",
        "import os\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xT9jTbr_1CMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed=42\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "BATCH_SIZE=128\n",
        "AVAIL_GPUS=min(1,torch.cuda.device_count())\n",
        "NUM_WORKERS=int(os.cpu_count()/2)"
      ],
      "metadata": {
        "id": "Va6YyE6t1CQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTDataModule(pl.LightningDataModule):\n",
        "\n",
        "  def __init__(self, data_dir=\"./data\", batch_size=BATCH_SIZE,\n",
        "               num_workers=NUM_WORKERS):\n",
        "    super().__init__()\n",
        "    self.data_dir=data_dir\n",
        "    self.batch_size=batch_size\n",
        "    self.num_workers=num_workers\n",
        "\n",
        "    self.transform=transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,),(0.3081,)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "  def prepare_data(self):\n",
        "    MNIST(self.data_dir, train=True, download=True)\n",
        "    MNIST(self.data_dir, train=False, download=True)\n",
        "\n",
        "\n",
        "  def setup(self, stage=None):\n",
        "    #assign trail/val datasets\n",
        "    if stage == \"fit\" or stage is None:\n",
        "      mnist_full=MNIST(self.data_dir, train=True, transform=self.transform)\n",
        "      self.mnist_train, self.mnist_val=random_split(mnist_full, [55000,5000])\n",
        "\n",
        "    #assign test datasets\n",
        "    if stage ==\"test\" or stage is None:\n",
        "      self.mnist_test=MNIST(self.data_dir, train=False, transform=self.transform)\n",
        "\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.mnist_train, self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.mnist_val, self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.mnist_test, self.batch_size, num_workers=self.num_workers)\n",
        "\n"
      ],
      "metadata": {
        "id": "QluQGtbw1CT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    #simple cnn\n",
        "    self.conv1=nn.Conv2d(1,10,kernel_size=5)\n",
        "    self.conv2=nn.Conv2d(10,20, kernel_size=5)\n",
        "    self.conv2_drop=nn.Dropout2d()\n",
        "    self.fc1=nn.Linear(320,50)\n",
        "    self.fc2=nn.Linear(50,1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x=F.relu(F.max_pool2d(self.conv1(x),2)) #(28,28)->(24,24)->(12,12)\n",
        "    x=F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)),2)) #(12,12)->(8,8)->(4,4)\n",
        "    x=x.view(-1, 320)\n",
        "    x=F.relu(self.fc1(x))\n",
        "    x=F.dropout(x, training=self.training)\n",
        "    x=self.fc2(x)\n",
        "   # return torch.sigmoid(x)\n",
        "    return x #for WGAN\n",
        "\n"
      ],
      "metadata": {
        "id": "XXEUAnOE1CWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Fake Data: output like real data [1, 28, 28] and values -1, 1\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(latent_dim, 7*7*64)  # [n, 64, 7, 7]\n",
        "        self.ct1 = nn.ConvTranspose2d(64, 32, 4, stride=2) # [n, 64, 16, 16]\n",
        "        self.ct2 = nn.ConvTranspose2d(32, 16, 4, stride=2) # [n, 16, 34, 34]\n",
        "        self.conv = nn.Conv2d(16, 1, kernel_size=7)  # [n, 1, 28, 28]\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass latent space input into linear layer and reshape\n",
        "        x = self.lin1(x)\n",
        "        x = F.relu(x)\n",
        "        x = x.view(-1, 64, 7, 7)\n",
        "\n",
        "        # Upsample (transposed conv) 16x16 (64 feature maps)\n",
        "        x = self.ct1(x) #(n, 64, 7, 7)->(n,32, 16, 16)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Upsample to 34x34 (16 feature maps)\n",
        "        x = self.ct2(x) #(n,32, 16, 16)->(n,16,34,34)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Convolution to 28x28 (1 feature map)\n",
        "        return self.conv(x) #(n,16,34,34)->(n,1,28,28)"
      ],
      "metadata": {
        "id": "MekjQ8zt1Cwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WGAN(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, latent_dim=100, lr=0.0001, gp_lambda=10): # Reduced LR for WGAN\n",
        "        super().__init__()\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.generator = Generator(latent_dim=self.hparams.latent_dim)\n",
        "        self.discriminator = Discriminator() # Discriminator is now a critic\n",
        "\n",
        "        self.validation_z = torch.rand(6, self.hparams.latent_dim)\n",
        "\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "    # Implement the gradient penalty\n",
        "    def compute_gradient_penalty(self, real_samples, fake_samples):\n",
        "        alpha = torch.randn((real_samples.size(0), 1, 1, 1), device=self.device)\n",
        "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "        d_interpolates = self.discriminator(interpolates)\n",
        "        gradients = autograd.grad(outputs=d_interpolates, inputs=interpolates,\n",
        "                                  grad_outputs=torch.ones(d_interpolates.size(), device=self.device),\n",
        "                                  create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "        gradients = gradients.view(gradients.size(0), -1)\n",
        "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "        return gradient_penalty\n",
        "\n",
        "    def forward(self, z):\n",
        "      return self.generator(z)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        optim_g, optim_d = self.optimizers()\n",
        "\n",
        "        real_imgs, _ = batch\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        # Train Discriminator (Critic)\n",
        "        optim_d.zero_grad()\n",
        "        z = torch.randn(batch_size, self.hparams.latent_dim, device=self.device)\n",
        "        fake_imgs = self(z).detach()\n",
        "\n",
        "        real_validity = self.discriminator(real_imgs)\n",
        "        fake_validity = self.discriminator(fake_imgs)\n",
        "\n",
        "        # Wasserstein loss\n",
        "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity)\n",
        "\n",
        "        # Gradient penalty\n",
        "        gradient_penalty = self.compute_gradient_penalty(real_imgs.data, fake_imgs.data)\n",
        "        d_loss = d_loss + self.hparams.gp_lambda * gradient_penalty\n",
        "\n",
        "        self.manual_backward(d_loss)\n",
        "        optim_d.step()\n",
        "\n",
        "        # Train Generator\n",
        "        # Only train the generator every N discriminator steps\n",
        "        if batch_idx % 5 == 0: # Example: train generator every 5 discriminator steps\n",
        "          optim_g.zero_grad()\n",
        "          z = torch.randn(batch_size, self.hparams.latent_dim, device=self.device)\n",
        "          fake_imgs = self(z)\n",
        "          fake_validity = self.discriminator(fake_imgs)\n",
        "\n",
        "          # Generator loss (minimize -D(fake_images))\n",
        "          g_loss = -torch.mean(fake_validity)\n",
        "\n",
        "          self.manual_backward(g_loss)\n",
        "          optim_g.step()\n",
        "\n",
        "          self.log_dict({\"g_loss\": g_loss}, prog_bar=True)\n",
        "\n",
        "\n",
        "        self.log_dict({\"d_loss\": d_loss}, prog_bar=True)\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.hparams.lr\n",
        "        optim_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "        optim_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "        return [optim_g, optim_d], []\n",
        "\n",
        "    def plot_imgs(self):\n",
        "        print(\"plot_imgs called\")\n",
        "        z = self.validation_z.type_as(self.generator.lin1.weight)\n",
        "        sample_imgs = self.forward(z).cpu()\n",
        "\n",
        "        print(f\"epoch {self.current_epoch}\")\n",
        "        fig = plt.figure()\n",
        "        for i in range(sample_imgs.size(0)):\n",
        "            plt.subplot(2, 3, i + 1)\n",
        "            plt.tight_layout()\n",
        "            # Slice to remove the channel dimension\n",
        "            plt.imshow(sample_imgs.detach()[i, 0, :, :], cmap=\"gray_r\", interpolation='none')\n",
        "            plt.title('Generated Data')\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        print(\"onepcohcend\")\n",
        "        self.plot_imgs()\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        print(\"training_epoch_end called\")\n",
        "        self.on_epoch_end()"
      ],
      "metadata": {
        "id": "5opxBfS5p1N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dm=MNISTDataModule()\n",
        "model2=WGAN()"
      ],
      "metadata": {
        "id": "SEg56EL0zb7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.plot_imgs()"
      ],
      "metadata": {
        "id": "3s9Fnc9MzcB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer=pl.Trainer(max_epochs=40, accelerator=\"gpu\", devices=AVAIL_GPUS)"
      ],
      "metadata": {
        "id": "6YAn7w96zcEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model2, dm)\n"
      ],
      "metadata": {
        "id": "APDD8tgy1C2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r7x1EnLt1C4y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}