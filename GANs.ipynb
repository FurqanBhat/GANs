{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOK+n+lT7RfqqFhuC8NyAjA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FurqanBhat/GANs/blob/main/GANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kSyd9AmvP8z"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import MNIST\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "import torch.autograd as autograd\n",
        "\n",
        "import os\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xT9jTbr_1CMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed=42\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "BATCH_SIZE=128\n",
        "AVAIL_GPUS=min(1,torch.cuda.device_count())\n",
        "NUM_WORKERS=int(os.cpu_count()/2)"
      ],
      "metadata": {
        "id": "Va6YyE6t1CQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTDataModule(pl.LightningDataModule):\n",
        "\n",
        "  def __init__(self, data_dir=\"./data\", batch_size=BATCH_SIZE,\n",
        "               num_workers=NUM_WORKERS):\n",
        "    super().__init__()\n",
        "    self.data_dir=data_dir\n",
        "    self.batch_size=batch_size\n",
        "    self.num_workers=num_workers\n",
        "\n",
        "    self.transform=transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,),(0.3081,)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "  def prepare_data(self):\n",
        "    MNIST(self.data_dir, train=True, download=True)\n",
        "    MNIST(self.data_dir, train=False, download=True)\n",
        "\n",
        "\n",
        "  def setup(self, stage=None):\n",
        "    #assign trail/val datasets\n",
        "    if stage == \"fit\" or stage is None:\n",
        "      mnist_full=MNIST(self.data_dir, train=True, transform=self.transform)\n",
        "      self.mnist_train, self.mnist_val=random_split(mnist_full, [55000,5000])\n",
        "\n",
        "    #assign test datasets\n",
        "    if stage ==\"test\" or stage is None:\n",
        "      self.mnist_test=MNIST(self.data_dir, train=False, transform=self.transform)\n",
        "\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.mnist_train, self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.mnist_val, self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.mnist_test, self.batch_size, num_workers=self.num_workers)\n",
        "\n"
      ],
      "metadata": {
        "id": "QluQGtbw1CT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    #simple cnn\n",
        "    self.conv1=nn.Conv2d(1,10,kernel_size=5)\n",
        "    self.conv2=nn.Conv2d(10,20, kernel_size=5)\n",
        "    self.conv2_drop=nn.Dropout2d()\n",
        "    self.fc1=nn.Linear(320,50)\n",
        "    self.fc2=nn.Linear(50,1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x=F.relu(F.max_pool2d(self.conv1(x),2)) #(28,28)->(24,24)->(12,12)\n",
        "    x=F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)),2)) #(12,12)->(8,8)->(4,4)\n",
        "    x=x.view(-1, 320)\n",
        "    x=F.relu(self.fc1(x))\n",
        "    x=F.dropout(x, training=self.training)\n",
        "    x=self.fc2(x)\n",
        "    return torch.sigmoid(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XXEUAnOE1CWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Fake Data: output like real data [1, 28, 28] and values -1, 1\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(latent_dim, 7*7*64)  # [n, 64, 7, 7]\n",
        "        self.ct1 = nn.ConvTranspose2d(64, 32, 4, stride=2) # [n, 64, 16, 16]\n",
        "        self.ct2 = nn.ConvTranspose2d(32, 16, 4, stride=2) # [n, 16, 34, 34]\n",
        "        self.conv = nn.Conv2d(16, 1, kernel_size=7)  # [n, 1, 28, 28]\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass latent space input into linear layer and reshape\n",
        "        x = self.lin1(x)\n",
        "        x = F.relu(x)\n",
        "        x = x.view(-1, 64, 7, 7)\n",
        "\n",
        "        # Upsample (transposed conv) 16x16 (64 feature maps)\n",
        "        x = self.ct1(x) #(n, 64, 7, 7)->(n,32, 16, 16)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Upsample to 34x34 (16 feature maps)\n",
        "        x = self.ct2(x) #(n,32, 16, 16)->(n,16,34,34)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Convolution to 28x28 (1 feature map)\n",
        "        return self.conv(x) #(n,16,34,34)->(n,1,28,28)"
      ],
      "metadata": {
        "id": "MekjQ8zt1Cwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class GAN(pl.LightningModule):\n",
        "\n",
        "#   def __init__(self, latent_dim=100, lr=0.0002):\n",
        "#     super().__init__()\n",
        "\n",
        "#     self.save_hyperparameters()\n",
        "\n",
        "#     self.generator=Generator(latent_dim=self.hparams.latent_dim)\n",
        "#     self.discriminator=Discriminator()\n",
        "\n",
        "#     #random noise\n",
        "#     self.validation_z=torch.rand(6, self.hparams.latent_dim)\n",
        "\n",
        "\n",
        "#   def forward(self, z):\n",
        "#     return self.generator(z)\n",
        "\n",
        "#   def adversarial_loss(self, y_hat, y):\n",
        "#     return F.binary_cross_entropy(y_hat, y)\n",
        "\n",
        "\n",
        "#   def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "#     real_imgs, _=batch\n",
        "\n",
        "#     #sample noise\n",
        "#     z=torch.randn(real_imgs.shape[0], self.hparams.latent_dim)\n",
        "#     z=z.type_as(real_imgs)\n",
        "\n",
        "#     #train generator : max log(D(G(z)))\n",
        "#     if optimizer_idx == 0:\n",
        "#       fake_imgs=self(z)\n",
        "#       y_hat=self.discriminator(fake_imgs)\n",
        "#       y=torch.ones(real_imgs.size(0), 1)\n",
        "#       y=y.type_as(real_imgs)\n",
        "\n",
        "#       g_loss= self.adversarial_loss(y_hat, y)\n",
        "\n",
        "#       log_dict={\"g_loss\": g_loss}\n",
        "\n",
        "#       return {\"loss\":g_loss, \"progress_bar\": log_dict, \"log\":log_dict}\n",
        "\n",
        "\n",
        "#     #train discriminator: max log(G(x))+log(1-D(G(z)))\n",
        "#     if optimizer_idx==1:\n",
        "#       #how well can it label as real\n",
        "#       y_hat_real=self.discriminator(real_imgs)\n",
        "#       y_real=torch.ones(real_imgs.size(0), 1)\n",
        "#       y_real=y_real.type_as(real_imgs)\n",
        "#       d_real_loss=self.adversarial_loss(y_hat_real, y_real)\n",
        "\n",
        "#       #how well can it label fake\n",
        "#       fake_imgs=self(z).detach()\n",
        "#       y_hat_fake=self.discriminator(fake_imgs)\n",
        "#       y_fake=torch.zeros(real_imgs.size(0),1)\n",
        "#       y_fake=y_fake.type_as(real_imgs)\n",
        "#       d_fake_loss=self.adversarial_loss(y_hat_fake, y_fake)\n",
        "\n",
        "#       d_loss=(d_real_loss+d_fake_loss)/2\n",
        "\n",
        "#       log_dict={\"d_loss\": d_loss}\n",
        "\n",
        "#       return {\"loss\":d_loss, \"progress_bar\": log_dict, \"log\":log_dict}\n",
        "\n",
        "\n",
        "\n",
        "#   def configure_optimizers(self):\n",
        "#     lr=self.hparams.lr\n",
        "#     optim_g=torch.optim.Adam(self.generator.parameters(), lr=lr)\n",
        "#     optim_d=torch.optim.Adam(self.discriminator.parameters(), lr=lr)\n",
        "#     return [optim_g, optim_d], []\n",
        "\n",
        "\n",
        "\n",
        "#   def plot_imgs(self):\n",
        "#     z=self.validation_z.type_as(self.generator.lin1.weight)\n",
        "#     sample_imgs=self.forward(z).cpu()\n",
        "\n",
        "#     print(f\"epoch {self.current_epoch}\")\n",
        "#     fig=plt.figure()\n",
        "#     for i in range(sample_imgs.size(0)):\n",
        "#       plt.subplot(2,3,i+1)\n",
        "#       plt.tight_layout()\n",
        "#       # Fix: Slice to remove the channel dimension\n",
        "#       plt.imshow(sample_imgs.detach()[i, 0, :, :], cmap=\"gray_r\", interpolation='none')\n",
        "#       plt.title('Generated Data')\n",
        "#       plt.xticks([])\n",
        "#       plt.yticks([])\n",
        "#       plt.axis('off')\n",
        "\n",
        "#     plt.show()\n",
        "\n",
        "#   def on_epoch_end(self):\n",
        "#     self.plot_imgs()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AXV5GQq11Czz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using manual optimization\n",
        "class GAN(pl.LightningModule):\n",
        "\n",
        "  def __init__(self, latent_dim=100, lr=0.0002):\n",
        "    super().__init__()\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    self.generator=Generator(latent_dim=self.hparams.latent_dim)\n",
        "    self.discriminator=Discriminator()\n",
        "\n",
        "    #random noise\n",
        "    self.validation_z=torch.rand(6, self.hparams.latent_dim)\n",
        "\n",
        "    # Set to False for manual optimization with multiple optimizers\n",
        "    self.automatic_optimization = False\n",
        "\n",
        "\n",
        "  def forward(self, z):\n",
        "    return self.generator(z)\n",
        "\n",
        "  def adversarial_loss(self, y_hat, y):\n",
        "    return F.binary_cross_entropy(y_hat, y)\n",
        "\n",
        "\n",
        "  # Removed optimizer_idx argument\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    # Manually access optimizers\n",
        "    optim_g, optim_d = self.optimizers()\n",
        "\n",
        "    real_imgs, _=batch\n",
        "\n",
        "    #sample noise\n",
        "    z=torch.randn(real_imgs.shape[0], self.hparams.latent_dim)\n",
        "    z=z.type_as(real_imgs)\n",
        "\n",
        "    #train generator : max log(D(G(z)))\n",
        "    # Zero the generator gradients\n",
        "    optim_g.zero_grad()\n",
        "    fake_imgs=self(z)\n",
        "    y_hat=self.discriminator(fake_imgs)\n",
        "    y=torch.ones(real_imgs.size(0), 1)\n",
        "    y=y.type_as(real_imgs)\n",
        "\n",
        "    g_loss= self.adversarial_loss(y_hat, y)\n",
        "\n",
        "    # Manual backward pass for generator\n",
        "    self.manual_backward(g_loss)\n",
        "    # Step the generator optimizer\n",
        "    optim_g.step()\n",
        "\n",
        "\n",
        "    #train discriminator: max log(G(x))+log(1-D(G(z)))\n",
        "    # Zero the discriminator gradients\n",
        "    optim_d.zero_grad()\n",
        "    #how well can it label as real\n",
        "    y_hat_real=self.discriminator(real_imgs)\n",
        "    y_real=torch.ones(real_imgs.size(0), 1)\n",
        "    y_real=y_real.type_as(real_imgs)\n",
        "    d_real_loss=self.adversarial_loss(y_hat_real, y_real)\n",
        "\n",
        "    #how well can it label fake\n",
        "    fake_imgs=self(z).detach()\n",
        "    y_hat_fake=self.discriminator(fake_imgs)\n",
        "    y_fake=torch.zeros(real_imgs.size(0),1)\n",
        "    y_fake=y_fake.type_as(real_imgs)\n",
        "    d_fake_loss=self.adversarial_loss(y_hat_fake, y_fake)\n",
        "\n",
        "    d_loss=(d_real_loss+d_fake_loss)/2\n",
        "\n",
        "    # Manual backward pass for discriminator\n",
        "    self.manual_backward(d_loss)\n",
        "    # Step the discriminator optimizer\n",
        "    optim_d.step()\n",
        "\n",
        "    # Log losses\n",
        "    self.log_dict({\"g_loss\": g_loss, \"d_loss\": d_loss}, prog_bar=True)\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    lr=self.hparams.lr\n",
        "    optim_g=torch.optim.Adam(self.generator.parameters(), lr=lr)\n",
        "    optim_d=torch.optim.Adam(self.discriminator.parameters(), lr=lr)\n",
        "    return [optim_g, optim_d], []\n",
        "\n",
        "\n",
        "  def plot_imgs(self):\n",
        "    print(\"plot_imgs called\")\n",
        "    z=self.validation_z.type_as(self.generator.lin1.weight)\n",
        "    sample_imgs=self.forward(z).cpu()\n",
        "\n",
        "    print(f\"epoch {self.current_epoch}\")\n",
        "    fig=plt.figure()\n",
        "    for i in range(sample_imgs.size(0)):\n",
        "      plt.subplot(2,3,i+1)\n",
        "      plt.tight_layout()\n",
        "      plt.imshow(sample_imgs.detach()[i, 0, :, :], cmap=\"gray_r\", interpolation='none')\n",
        "      plt.title('Generated Data')\n",
        "      plt.xticks([])\n",
        "      plt.yticks([])\n",
        "      plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    print(\"onepcohcend\")\n",
        "    self.plot_imgs()\n",
        "\n",
        "  def on_train_epoch_end(self):\n",
        "    print(\"training_epoch_end called\")\n",
        "    self.on_epoch_end()\n"
      ],
      "metadata": {
        "id": "24Ay88n6Fhf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dm=MNISTDataModule()\n",
        "model1=GAN()\n"
      ],
      "metadata": {
        "id": "SEg56EL0zb7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.plot_imgs()"
      ],
      "metadata": {
        "id": "3s9Fnc9MzcB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer=pl.Trainer(max_epochs=40, accelerator=\"gpu\", devices=AVAIL_GPUS)"
      ],
      "metadata": {
        "id": "6YAn7w96zcEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model1, dm)"
      ],
      "metadata": {
        "id": "deOX9wwmsVuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r7x1EnLt1C4y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}